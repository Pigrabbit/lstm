{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import dirname, join as pjoin\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "DATA_PATH = './data'\n",
    "DATA_FILE_NAME = 'Brugge_en_d.mat'\n",
    "RESULT_FILE_NAME = 'rmse_brugge.csv'\n",
    "\n",
    "IMAGE_PATH = './images'\n",
    "\n",
    "data_dir = pjoin(DATA_PATH, DATA_FILE_NAME)\n",
    "mat_contents = sio.loadmat(data_dir)\n",
    "\n",
    "data = mat_contents['en_d'][0, 0]\n",
    "TRUE_MODEL_INDEX = 103\n",
    "NUM_WELL = 20       # P1-20\n",
    "NUM_MODEL = 104         # 1-104\n",
    "\n",
    "INPUT_SEQUENCE = 5\n",
    "OUTPUT_SEQUENCE = 1 \n",
    "TRAIN_SPLIT = 150\n",
    "INPUT_DIMENSION = 4     # num of input features\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 5000\n",
    "LSTM_NUM_UNITS = 50\n",
    "LSTM_LOSS = 'mae'\n",
    "LSTM_OPTIMIZER = 'adam'\n",
    "EPOCHS = 50\n",
    "EVALUATION_INTERVAL = 200\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is tf built with cuda:  True\n",
      "GPU Available:  True\n"
     ]
    }
   ],
   "source": [
    "# Check gpu availability\n",
    "print(\"is tf built with cuda: \", tf.test.is_built_with_cuda())\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "# print(device_lib.list_local_devices())\n",
    "# print(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dic_of_df(data, num_producer, num_model):\n",
    "    well_dic = {}\n",
    "    for well_index in range(num_producer):      # well, Producer P1-P20\n",
    "        # 'model_num' => dataframe\n",
    "        model_dic = {}\n",
    "        well_key = 'P' + str(well_index+1)\n",
    "        for model_index in range(num_model):    # model, model 1-104\n",
    "            well_data = np.array([\n",
    "                data['WOPR'][0,0][well_key][:,model_index],\n",
    "                data['WBHP'][0,0][well_key][:,model_index],\n",
    "                data['WWCT'][0,0][well_key][:,model_index],\n",
    "                data['WWPR'][0,0][well_key][:,model_index]\n",
    "              ])\n",
    "            # convert np array to dataframe\n",
    "            # | date | WOPR | WBHP | WWCT | WWPR |\n",
    "            # |------|------|------|------|------|\n",
    "            # |  0.0 |  0.0 |  0.0 |  0.0 |  0.0 |\n",
    "            # | .... | .... | .... | .... | .... |  \n",
    "            # |3648.0| .... | .... | .... | .... |  \n",
    "            well_data = well_data.T\n",
    "            df = pd.DataFrame(\n",
    "                data=well_data,\n",
    "                index=data['TIME'].flatten(),\n",
    "                columns=['WOPR', 'WBHP', 'WWCT', 'WWPR']\n",
    "            )\n",
    "            df.index.name = 'date'\n",
    "            model_dic[str(model_index+1)] = df\n",
    "\n",
    "        well_dic[str(well_index+1)] = model_dic\n",
    "    return well_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(dataframe, n_in=1, n_out=1, dropnan=True):\n",
    "    '''\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        dataframe: Sequence of observations as a dataframe\n",
    "        n_in: Number of lag observation as input, number of sequence\n",
    "        n_out: Number of observations as output, output dimension\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    '''\n",
    "    n_vars = len(dataframe.columns)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ..., t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(dataframe.shift(i))\n",
    "        names += [('var%d(t-%d)' %(j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, ... , t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(dataframe.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1,i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_index(time_df, train_split):\n",
    "    return time_df[train_split-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(scaler, target_df, date_index):\n",
    "    # Get a dataframe as parameter and minmax scale it\n",
    "    # and return the result dataframe\n",
    "    reframed = series_to_supervised(target_df, n_in=INPUT_SEQUENCE, n_out= OUTPUT_SEQUENCE)\n",
    "    scaler = scaler.fit(reframed.loc[:date_index,:])\n",
    "    reframed.loc[:, :] = scaler.transform(reframed.loc[:, :])\n",
    "    return reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(reframed_df):\n",
    "    reframed_np = reframed_df.to_numpy()\n",
    "    train, test = reframed_np[:TRAIN_SPLIT, :], reframed_np[TRAIN_SPLIT:, :]\n",
    "    # last 4 columns are data of cuurrent time step\n",
    "    train_X, train_y = train[:, :-4], train[:, -4]\n",
    "    test_X, test_y = test[:, :-4], test[:, -4]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], INPUT_SEQUENCE, INPUT_DIMENSION))\n",
    "    test_X = test_X.reshape((test_X.shape[0], INPUT_SEQUENCE, INPUT_DIMENSION))\n",
    "    return train_X, train_y, test_X, test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_history(history, title, fig_id, fig_extension='png', resolution=300):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    \n",
    "    path = pjoin(IMAGE_PATH, fig_id + \".\" + fig_extension)  \n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction(eval_array, time_df, scale, title, fig_id, fig_extension='png', resolution=300):\n",
    "    df = pd.DataFrame(data=eval_array.T, columns=['prediction', 'true'])\n",
    "    \n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(time_df[5:], df[['prediction']] * scale, linestyle='-', label='prediction')\n",
    "    plt.plot(time_df[5:], df[['true']] * scale, linestyle='-',label = 'true')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    path = pjoin(IMAGE_PATH, fig_id + \".\" + fig_extension)\n",
    "    plt.savefig(path, format=fig_extension, dip=resolution)\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(y_prediction, y_true, scaler, reframed):\n",
    "    # invert scaling for forecast\n",
    "    test = reframed.to_numpy()[TRAIN_SPLIT:, :]\n",
    "\n",
    "    inv_y_hat = np.concatenate((y_prediction, test[:, 1:]), axis = 1)\n",
    "    inv_y_hat = scaler.inverse_transform(inv_y_hat)\n",
    "    inv_y_hat = inv_y_hat[:, 0]\n",
    "\n",
    "    y_true = y_true.reshape((len(y_true), 1))\n",
    "    inv_y = np.concatenate((y_true, test[:, 1:]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:, 0]\n",
    "\n",
    "    # calculate RMSE\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_y_hat))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op TensorSliceDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousMemoryCache in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op CacheDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousRandomSeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ShuffleDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op BatchDatasetV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Qr in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DiagPart in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sign in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Transpose in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ConcatV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DatasetCardinality in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Train for 200 steps, validate for 50 steps\n",
      "Epoch 1/50\n",
      "Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_1905 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_distributed_function_3115 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.0713Executing op __inference_distributed_function_4040 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "200/200 [==============================] - 12s 62ms/step - loss: 0.0711 - val_loss: 0.3078\n",
      "Epoch 2/50\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 0.0428Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.0427 - val_loss: 0.2488\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.0346 - val_loss: 0.1778\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.0325 - val_loss: 0.1642\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.0290 - val_loss: 0.1594\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.0250 - val_loss: 0.2051\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.0250 - val_loss: 0.1533\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.0240 - val_loss: 0.1473\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.0227 - val_loss: 0.1824\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.0214 - val_loss: 0.1360\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.0201 - val_loss: 0.1611\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.0199 - val_loss: 0.1322\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.0185 - val_loss: 0.1516\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.0183 - val_loss: 0.1563\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.0201 - val_loss: 0.1295\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.0167 - val_loss: 0.1333\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0177 - val_loss: 0.1348\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0175 - val_loss: 0.1571\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0160 - val_loss: 0.1516\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0151 - val_loss: 0.1566\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0151 - val_loss: 0.1479\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 3s 14ms/step - loss: 0.0154 - val_loss: 0.1518\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0156 - val_loss: 0.1703\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0152 - val_loss: 0.1859\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0144 - val_loss: 0.1874\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0148 - val_loss: 0.1836\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0139 - val_loss: 0.1654\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0145 - val_loss: 0.1802\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0149 - val_loss: 0.1633\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0142 - val_loss: 0.2030\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0140 - val_loss: 0.1838\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0134 - val_loss: 0.1860\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0135 - val_loss: 0.1830\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0138 - val_loss: 0.1833\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0125 - val_loss: 0.1988\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0132 - val_loss: 0.1922\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0134 - val_loss: 0.1940\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0125 - val_loss: 0.2010\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0128 - val_loss: 0.1833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0127 - val_loss: 0.1872\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.0125 - val_loss: 0.1985\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0126 - val_loss: 0.2084\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0122 - val_loss: 0.1898\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0130 - val_loss: 0.2095\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0128 - val_loss: 0.1953\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0119 - val_loss: 0.1824\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0120 - val_loss: 0.1960\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0129 - val_loss: 0.2001\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0128 - val_loss: 0.1877\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0120 - val_loss: 0.1855\n",
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_29422 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "RMSE: 374.5143423155855\n",
      "Executing op DeleteMemoryCache in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op DeleteRandomSeedGenerator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Train for 200 steps, validate for 50 steps\n",
      "Epoch 1/50\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_31339 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_distributed_function_32549 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "190/200 [===========================>..] - ETA: 0s - loss: 0.0829Executing op __inference_distributed_function_33474 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 0.0816 - val_loss: 0.3912\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0492 - val_loss: 0.2836\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0397 - val_loss: 0.2407\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0323 - val_loss: 0.2197\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0279 - val_loss: 0.1965\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0248 - val_loss: 0.2147\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0230 - val_loss: 0.1939\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0214 - val_loss: 0.1359\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0194 - val_loss: 0.1675\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0194 - val_loss: 0.1651\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0180 - val_loss: 0.1604\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0180 - val_loss: 0.1579\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0172 - val_loss: 0.1547\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0168 - val_loss: 0.1480\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0166 - val_loss: 0.1424\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0169 - val_loss: 0.1470\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0166 - val_loss: 0.1425\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0156 - val_loss: 0.1368\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0152 - val_loss: 0.1348\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0149 - val_loss: 0.1348\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0159 - val_loss: 0.1303\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0147 - val_loss: 0.1183\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0148 - val_loss: 0.1125\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0153 - val_loss: 0.1162\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0142 - val_loss: 0.1138\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0158 - val_loss: 0.1145\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0154 - val_loss: 0.1120\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0147 - val_loss: 0.1116\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0131 - val_loss: 0.1129\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0139 - val_loss: 0.1069\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0141 - val_loss: 0.1075\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0130 - val_loss: 0.1074\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0136 - val_loss: 0.1033\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0137 - val_loss: 0.1051\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0133 - val_loss: 0.1030\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0121 - val_loss: 0.1025\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0135 - val_loss: 0.1018\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0139 - val_loss: 0.1032\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0138 - val_loss: 0.1053\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0127 - val_loss: 0.1039\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0125 - val_loss: 0.1013\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0127 - val_loss: 0.1019\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0125 - val_loss: 0.1033\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0129 - val_loss: 0.1023\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0124 - val_loss: 0.1015\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0126 - val_loss: 0.1008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0127 - val_loss: 0.1090\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0127 - val_loss: 0.1159\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0141 - val_loss: 0.1145\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0119 - val_loss: 0.1091\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_distributed_function_58856 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "RMSE: 221.68965580133712\n",
      "Train for 200 steps, validate for 50 steps\n",
      "Epoch 1/50\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_initialize_variables_60773 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_distributed_function_61983 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.0739Executing op __inference_distributed_function_62908 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "200/200 [==============================] - 6s 30ms/step - loss: 0.0729 - val_loss: 0.4405\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0497 - val_loss: 0.4318\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0398 - val_loss: 0.3863\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0333 - val_loss: 0.2194\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0293 - val_loss: 0.1536\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0258 - val_loss: 0.1254\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0239 - val_loss: 0.0317\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0226 - val_loss: 0.0148\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0211 - val_loss: 0.0452\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0208 - val_loss: 0.0469\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0195 - val_loss: 0.0423\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0192 - val_loss: 0.0280\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0187 - val_loss: 0.0521\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0180 - val_loss: 0.0454\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0181 - val_loss: 0.0497\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0174 - val_loss: 0.0518\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0165 - val_loss: 0.0584\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0164 - val_loss: 0.0407\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0171 - val_loss: 0.0393\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0163 - val_loss: 0.0333\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0160 - val_loss: 0.0522\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0160 - val_loss: 0.0316\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0157 - val_loss: 0.0524\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0158 - val_loss: 0.0381\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0155 - val_loss: 0.0450\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0149 - val_loss: 0.0320\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0147 - val_loss: 0.0149\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0157 - val_loss: 0.0130\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0135 - val_loss: 0.0092\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0138 - val_loss: 0.0082\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0142 - val_loss: 0.0142\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0139 - val_loss: 0.0260\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0138 - val_loss: 0.0169\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0140 - val_loss: 0.0142\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0145 - val_loss: 0.0047\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0136 - val_loss: 0.0206\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0132 - val_loss: 0.0077\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0144 - val_loss: 0.0139\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0122 - val_loss: 0.0058\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0131 - val_loss: 0.0039\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0130 - val_loss: 0.0043\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0128 - val_loss: 0.0054\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0136 - val_loss: 0.0065\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0136 - val_loss: 0.0092\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0129 - val_loss: 0.0067\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0127 - val_loss: 0.0164\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0124 - val_loss: 0.0218\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0119 - val_loss: 0.0241\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0123 - val_loss: 0.0173\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.0136 - val_loss: 0.0109\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_distributed_function_88290 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "RMSE: 25.12037179518473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "well_dic = to_dic_of_df(\n",
    "    data = data,\n",
    "    num_producer=NUM_WELL,\n",
    "    num_model=NUM_MODEL\n",
    ")\n",
    "scaler = MinMaxScaler()\n",
    "date_index = get_date_index(data['TIME'], TRAIN_SPLIT)\n",
    "\n",
    "for well_index in range(10,11): #range(1, NUM_WELL+1):\n",
    "    for model_index in range(1,4): #range(1, NUM_MODEL+1): \n",
    "        target_df = well_dic[str(well_index)][str(model_index)]\n",
    "        reframed = scale_dataset(scaler, target_df, date_index)\n",
    "        train_X, train_y, test_X, test_y = split_train_test(reframed)\n",
    "\n",
    "        train_data = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n",
    "        train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "        val_data = tf.data.Dataset.from_tensor_slices((test_X, test_y))\n",
    "        val_data = val_data.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "        # define and fit the model using keras\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(\n",
    "            units = LSTM_NUM_UNITS, \n",
    "            input_shape=(INPUT_SEQUENCE, INPUT_DIMENSION)\n",
    "        ))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.compile(loss = LSTM_LOSS, optimzer = LSTM_OPTIMIZER)\n",
    "        history = model.fit(\n",
    "            train_data,\n",
    "            epochs=EPOCHS,\n",
    "            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "            validation_data=val_data,\n",
    "            validation_steps=50,\n",
    "            use_multiprocessing=True,\n",
    "            workers=8\n",
    "            )\n",
    "\n",
    "        save_train_history(\n",
    "            history=history, \n",
    "            title=f\"Well {well_index} Model {model_index} training and validation loss\",\n",
    "            fig_id=f\"w{well_index}_m{model_index}_history\"\n",
    "            ) \n",
    "\n",
    "        # Evaluate the model\n",
    "        y_hat = model.predict(test_X)\n",
    "        test_X = test_X.reshape(test_X.shape[0], INPUT_SEQUENCE * INPUT_DIMENSION)\n",
    "\n",
    "        # scale factor of label\n",
    "        scale = 1 / scaler.scale_[-4]\n",
    "\n",
    "        evaluate_array = np.array([\n",
    "            np.concatenate((train_y, y_hat), axis=None),\n",
    "            np.concatenate((train_y, test_y), axis=None)\n",
    "            ])\n",
    "\n",
    "        time_df = data['TIME']\n",
    "\n",
    "        save_prediction(\n",
    "            eval_array=evaluate_array, \n",
    "            time_df=time_df,\n",
    "            scale=scale,\n",
    "            title=f\"Well {well_index} Model {model_index} prediction\",\n",
    "            fig_id=f\"w{well_index}_m{model_index}_prediction\"\n",
    "            )\n",
    "\n",
    "        rmse = calculate_rmse(\n",
    "            y_prediction=y_hat,\n",
    "            y_true=test_y,\n",
    "            scaler=scaler,\n",
    "            reframed=reframed\n",
    "        )\n",
    "\n",
    "        print(f\"RMSE: {rmse}\")\n",
    "        \n",
    "        dir = pjoin(DATA_PATH, RESULT_FILE_NAME)\n",
    "        f = open(dir, \"a\")\n",
    "        f.write(f\"{well_index}, {model_index}, {rmse}\\n\")\n",
    "        f.close()\n",
    "\n",
    "        tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
